{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3de16f2a",
      "metadata": {
        "id": "3de16f2a"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import lxml\n",
        "import os\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf \n",
        "import numpy as np \n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import cv2\n",
        "import matplotlib.patches as patches"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" setting up google drive \"\"\"\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "O9W_Zzccx4s-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4acc59ec-f7b5-4868-dc35-d6fb0ddeb0ba"
      },
      "id": "O9W_Zzccx4s-",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" defining data directories used later \"\"\"\n",
        "\n",
        "MOUNTPOINT = '/content/drive'\n",
        "DATADIR = os.path.join(MOUNTPOINT, 'MyDrive', 'Project_Face_Mask')\n",
        "DATADIR_annotations = os.path.join(DATADIR, 'Annotations')"
      ],
      "metadata": {
        "id": "0mgfRQlDx1wp"
      },
      "id": "0mgfRQlDx1wp",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" loading test / train datasets that have been generated using \"data_generation.ipynb\" \"\"\"\n",
        "\n",
        "ds_train = tf.data.experimental.load(os.path.join(DATADIR, 'train_ds_FRCNN'))\n",
        "ds_test = tf.data.experimental.load(os.path.join(DATADIR, 'test_ds_FRCNN'))"
      ],
      "metadata": {
        "id": "Hgordr_MNHxc"
      },
      "id": "Hgordr_MNHxc",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "11e41342",
      "metadata": {
        "id": "11e41342"
      },
      "outputs": [],
      "source": [
        "def prepare_mnist_data(ds):\n",
        "\n",
        "    \"\"\" convert data from uint8 to float32 \"\"\"\n",
        "    ds = ds.map(lambda img, label, boxs: (tf.cast(img, tf.float32), label, boxs))\n",
        "\n",
        "    \"\"\" convert images to grayscale \"\"\"\n",
        "    ds = ds.map(lambda img, label, boxs: (tf.image.rgb_to_grayscale(img), label, boxs))\n",
        "\n",
        "    \"\"\" create one-hot targets \"\"\"\n",
        "    ds = ds.map(lambda img, label, boxs: (img, tf.one_hot(label, depth=3), boxs))\n",
        "    \n",
        "    \"\"\" shuffle, batching only with only 1 sample due to memory reasons\"\"\"\n",
        "    ds = ds.shuffle(1000).batch(1)\n",
        "    \n",
        "    return ds\n",
        "\n",
        "ds_train = prepare_mnist_data(ds_train)\n",
        "ds_test = prepare_mnist_data(ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2d4447e2",
      "metadata": {
        "id": "2d4447e2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "class Partial_VGG(tf.keras.Model):\n",
        "    \"\"\" Backbone CNN to extract feature maps for further classification \"\"\"\n",
        "    def __init__(self): \n",
        "        super(Partial_VGG, self).__init__()\n",
        "        self.conv_1_1 = Conv2D(32, (3, 3), activation='relu', padding='same')\n",
        "        self.conv_1_2 = Conv2D(32, (3, 3), activation='relu', padding='same')\n",
        "        self.pool_1 = MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        \n",
        "        self.conv_2_1 = Conv2D(64, (3, 3), activation='relu', padding='same')\n",
        "        self.conv_2_2 = Conv2D(64, (3, 3), activation='relu', padding='same')\n",
        "        self.pool_2 = MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        \n",
        "        self.conv_3_1 = Conv2D(128, (3, 3), activation='relu', padding='same')\n",
        "        self.conv_3_2 = Conv2D(128, (3, 3), activation='relu', padding='same')\n",
        "        self.conv_3_3 = Conv2D(128, (3, 3), activation='relu', padding='same')\n",
        "        \n",
        "        self.pool_3 = MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        \n",
        "        self.conv_4_1 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
        "        self.conv_4_2 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
        "        self.conv_4_3 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
        "        self.pool_4 = MaxPooling2D((2, 2), strides=(2, 2))\n",
        "        \n",
        "        self.conv_5_1 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
        "        self.conv_5_2 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
        "        self.conv_5_3 = Conv2D(256, (3, 3), activation='relu', padding='same')\n",
        "        \n",
        "    def call(self, img):\n",
        "        feat_extr = self.conv_1_1(img)\n",
        "        feat_extr = self.conv_1_2(feat_extr)\n",
        "        feat_extr = self.pool_1(feat_extr)\n",
        "        \n",
        "        feat_extr = self.conv_2_1(feat_extr)\n",
        "        feat_extr = self.conv_2_2(feat_extr)\n",
        "        feat_extr = self.pool_2(feat_extr)\n",
        "        \n",
        "        feat_extr = self.conv_3_1(feat_extr)\n",
        "        feat_extr = self.conv_3_2(feat_extr)\n",
        "        feat_extr = self.conv_3_3(feat_extr)\n",
        "        \n",
        "        feat_extr = self.pool_3(feat_extr)\n",
        "        \n",
        "        feat_extr = self.conv_4_1(feat_extr)\n",
        "        feat_extr = self.conv_4_2(feat_extr)\n",
        "        feat_extr = self.conv_4_3(feat_extr)\n",
        "        feat_extr = self.pool_4(feat_extr)\n",
        "    \n",
        "        feat_extr = self.conv_5_1(feat_extr)\n",
        "        feat_extr = self.conv_5_2(feat_extr)\n",
        "        feat_extr = self.conv_5_3(feat_extr)\n",
        "        \n",
        "        return feat_extr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "55a15d2b",
      "metadata": {
        "id": "55a15d2b"
      },
      "outputs": [],
      "source": [
        "class RPN_Layer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Region Proposal Network which creates Region Proposals for object detection \n",
        "    \n",
        "        returns the input for the ROI Pooling after beeing transformed in the rpn_to_roi method\n",
        "    \"\"\"\n",
        "    def __init__(self, num_anchors): \n",
        "        super(RPN_Layer, self).__init__()\n",
        "        \n",
        "        \"\"\" backbone CNN layer before classification / regression \"\"\"\n",
        "        self.conv_1 = Conv2D(512, (3, 3), padding='same', activation='relu')\n",
        "        \n",
        "        \"\"\" classification whether an object is found or not \"\"\" \n",
        "        self.classif = Conv2D(num_anchors, (1, 1), activation='sigmoid')\n",
        "        \n",
        "        \"\"\" regression for proposed bounding boxes  \"\"\"\n",
        "        self.regress = Conv2D(num_anchors * 4, (1, 1), activation='linear')\n",
        "        \n",
        "    def call(self, feat_map):\n",
        "        feat_extr = self.conv_1(feat_map)\n",
        "        obj_class = self.classif(feat_extr)\n",
        "        box_regr = self.regress(feat_extr)\n",
        "        \n",
        "        return [obj_class, box_regr, feat_map]\n",
        "    \n",
        "class RoiPooling(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Pooling of the region proposals into a fixed size (7x7) in order to feed\n",
        "    them into the classification layer \n",
        "    \"\"\"\n",
        "    def __init__(self, pool_size, num_rois):\n",
        "        super(RoiPooling, self).__init__()\n",
        "        \n",
        "        self.pool_size = pool_size\n",
        "        self.num_rois = num_rois\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.nb_channels = input_shape[0][3]  \n",
        "\n",
        "    def call(self, img_rois):\n",
        "        img, rois = img_rois\n",
        "        input_shape = tf.shape(img)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        for nb_roi in range(self.num_rois):\n",
        "            \"\"\"\n",
        "            extracting coordinates of bounding boxes and resize the specific\n",
        "            region to the given pool size 7x7\n",
        "            \"\"\"\n",
        "            x_min = int(rois[nb_roi, 0])\n",
        "            y_min = int(rois[nb_roi, 1])\n",
        "            x_max = int(rois[nb_roi, 2])\n",
        "            y_max = int(rois[nb_roi, 3])      \n",
        "\n",
        "            pooled_pict = tf.image.resize(img[:, y_min:y_max, x_min:x_max, :], (self.pool_size, self.pool_size))\n",
        "            outputs.append(pooled_pict)           \n",
        "        \n",
        "        roi_results = np.concatenate(outputs, axis=0)\n",
        "        roi_results = np.expand_dims(roi_results, 0)\n",
        "\n",
        "        return roi_results\n",
        "\n",
        "class Class_Layer(tf.keras.layers.Layer):\n",
        "    \"\"\" Layer that that processes the output from RPN \"\"\"\n",
        "    def __init__(self, num_rois, nb_classes = 3): \n",
        "        super(Class_Layer, self).__init__()\n",
        "        \n",
        "        \"\"\" define the pool size \"\"\"\n",
        "        self.nb_pooling_regions = 7\n",
        "        \n",
        "        \"\"\" last part of the Faster RCNN architecture \"\"\"\n",
        "        self.roi_pool = RoiPooling(self.nb_pooling_regions, num_rois)\n",
        "        self.flatten = Flatten()\n",
        "        self.dense_1 = Dense(2048, activation='relu')\n",
        "        self.dropout_1 = Dropout(0.5)\n",
        "        self.dense_2 = Dense(2048, activation='relu')\n",
        "        self.dropout_2 = Dropout(0.5)\n",
        "        \n",
        "        \"\"\" \n",
        "        softmax classification for prediciting whether the specific image\n",
        "        with bounding box inherits a incorrectly worn mask, no mask or a correctly\n",
        "        worn mask\n",
        "        \"\"\"\n",
        "        self.classif = Dense(nb_classes, activation='softmax', kernel_initializer='zero')\n",
        "        \n",
        "        \"\"\" regression for the specific bounding box \"\"\"\n",
        "        self.regr = Dense(4, activation='linear', kernel_initializer='zero')\n",
        "    \n",
        "    def call(self, feat_map, input_rois):\n",
        "        feat_extr = self.roi_pool([feat_map, input_rois])\n",
        "        feat_extr = self.flatten(feat_extr)\n",
        "        feat_extr = self.dense_1(feat_extr)\n",
        "        feat_extr = self.dropout_1(feat_extr)\n",
        "        feat_extr = self.dense_2(feat_extr)\n",
        "        feat_extr = self.dropout_2(feat_extr)\n",
        "        \n",
        "        out_classif = self.classif(feat_extr)\n",
        "        out_regr = self.regr(feat_extr)\n",
        "        \n",
        "        return [out_classif, out_regr]\n",
        "\n",
        "class FasterRCNN(tf.keras.Model):\n",
        "  \"\"\" model that brings together all parts of the Faster RCNN \"\"\"\n",
        "  def __init__(self): \n",
        "      super(FasterRCNN, self).__init__()\n",
        "      self.vgg = Partial_VGG()\n",
        "      self.rpn = RPN_Layer(9)\n",
        "      self.classif = Class_Layer(16)\n",
        "  \n",
        "  def call(self, img):\n",
        "      feat_extr = self.vgg(img)\n",
        "      \n",
        "      x_class, x_regr, feat_map = self.rpn(feat_extr)\n",
        "      \n",
        "      regions_of_interest = rpn_to_roi(x_regr, x_class)\n",
        "      \n",
        "      out_class, out_regr = self.classif(feat_map, regions_of_interest)\n",
        "\n",
        "      return out_class, out_regr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1d0187fd",
      "metadata": {
        "id": "1d0187fd"
      },
      "outputs": [],
      "source": [
        "def apply_regr(anchor, regr):\n",
        "    \"\"\"\n",
        "    method that offests the regressed region proposals with the respective anchor \n",
        "    \n",
        "    Parameters:\n",
        "        anchor: position of the bounding box\n",
        "        regr: region proposal \n",
        "    \n",
        "    Returns:\n",
        "        x_min, y_min, width, height\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\" extracting coordinates from box/anchor \"\"\"\n",
        "    x = anchor[0, :, :]\n",
        "    y = anchor[1, :, :]\n",
        "    w = anchor[2, :, :]\n",
        "    h = anchor[3, :, :]\n",
        "\n",
        "    anch_x = regr[0, :, :]\n",
        "    anch_y = regr[1, :, :]\n",
        "    anch_w = regr[2, :, :]\n",
        "    anch_h = regr[3, :, :]\n",
        "    \n",
        "    \"\"\" calculating middle point of regr box \"\"\"\n",
        "    mx = x + w/2.\n",
        "    my = y + h/2.\n",
        "    \n",
        "    \"\"\" offsetting the regr box with the anchor box \"\"\"\n",
        "    mx1 = anch_x * w + mx\n",
        "    my1 = anch_y * h + my\n",
        "    \n",
        "    \"\"\" rounding and calculating the exponential / product\"\"\"\n",
        "    w1 = np.round(np.exp(anch_w.astype(np.float64)) * w)\n",
        "    h1 = np.round(np.exp(anch_h.astype(np.float64)) * h)\n",
        "    x1 = np.round(mx1 - w1/2.)\n",
        "    y1 = np.round(my1 - h1/2.)\n",
        "    \n",
        "    return np.stack([x1, y1, w1, h1])\n",
        " \n",
        "def rpn_to_roi(x_regr, x_class, max_boxes=16, rpn_stride=16):\n",
        "    \"\"\"\n",
        "    transform region proposals that they can be fed to the roi_pooling layer\n",
        "    \n",
        "    Parameters:\n",
        "        x_regr: bounding box proposals \n",
        "        x_class: classification probabilities\n",
        "        max_boxes: maximum boxes to extract per image \n",
        "        rpn_stride: summation of strides in backbone CNN\n",
        "    \n",
        "    Returns:\n",
        "        regions_of_interest: maximum max_boxes number of bounding boxes \n",
        "    \"\"\"\n",
        "    \n",
        "    anchor_no = 0\n",
        "    \n",
        "    \"\"\" define default anchor_sizes and anchor_ratios\"\"\"\n",
        "    anchor_sizes = [128, 256, 512]\n",
        "    anchor_ratios = [(1,1), (1,2*np.sqrt(2)), (2*np.sqrt(2),1)]\n",
        "    \n",
        "    \"\"\" define size of rpn table \"\"\"\n",
        "    (rows, cols) = x_class.shape[1:3]\n",
        "    calc_rpns = np.zeros((4, x_class.shape[1], x_class.shape[2], x_class.shape[3]))\n",
        "    \n",
        "    for anchor_size in anchor_sizes:\n",
        "        for anchor_ratio in anchor_ratios:\n",
        "            \"\"\" anchor_y = (128 * 2) / 16 = 16 => height of current anchor \"\"\" \n",
        "            anchor_x = (anchor_size * anchor_ratio[0]) / rpn_stride\n",
        "            anchor_y = (anchor_size * anchor_ratio[1]) / rpn_stride\n",
        "            \n",
        "            \"\"\" reshape bounding boxes \"\"\" \n",
        "            regr = x_regr[0, :, :, 4 * anchor_no:4 * anchor_no + 4] \n",
        "            regr = np.transpose(regr, (2, 0, 1)) \n",
        "            \n",
        "            \"\"\" creates N-D coordinate array for vectorized evaluations \"\"\"\n",
        "            X, Y = np.meshgrid(np.arange(cols),np.arange(rows))\n",
        "            \n",
        "            \"\"\" get coordinates, width and height of current anchor \"\"\"\n",
        "            calc_rpns[0, :, :, anchor_no] = X - anchor_x/2 \n",
        "            calc_rpns[1, :, :, anchor_no] = Y - anchor_y/2 \n",
        "            calc_rpns[2, :, :, anchor_no] = anchor_x      \n",
        "            calc_rpns[3, :, :, anchor_no] = anchor_y       \n",
        "            \n",
        "            \"\"\" offseting the anchor with the given regression boxes \"\"\"\n",
        "            calc_rpns[:, :, :, anchor_no] = apply_regr(calc_rpns[:, :, :, anchor_no], regr)\n",
        "            \n",
        "            \"\"\" width and height should be at maximum 1 \"\"\"\n",
        "            calc_rpns[2, :, :, anchor_no] = np.maximum(1, calc_rpns[2, :, :, anchor_no])\n",
        "            calc_rpns[3, :, :, anchor_no] = np.maximum(1, calc_rpns[3, :, :, anchor_no])\n",
        "            \n",
        "            \"\"\" get x_max and y_max by addding the height/width to the respective coordinates \"\"\"\n",
        "            calc_rpns[2, :, :, anchor_no] += calc_rpns[0, :, :, anchor_no]\n",
        "            calc_rpns[3, :, :, anchor_no] += calc_rpns[1, :, :, anchor_no]\n",
        "\n",
        "            \"\"\" make sure that the boundung boxes are inside the feature map \"\"\"\n",
        "            calc_rpns[0, :, :, anchor_no] = np.maximum(0, calc_rpns[0, :, :, anchor_no])\n",
        "            calc_rpns[1, :, :, anchor_no] = np.maximum(0, calc_rpns[1, :, :, anchor_no])\n",
        "            calc_rpns[2, :, :, anchor_no] = np.minimum(cols-1, calc_rpns[2, :, :, anchor_no])\n",
        "            calc_rpns[3, :, :, anchor_no] = np.minimum(rows-1, calc_rpns[3, :, :, anchor_no])\n",
        "            \n",
        "            anchor_no += 1\n",
        "    \n",
        "    \"\"\" reshaping the boxes and the probabilities into usable format (4050, N)\"\"\"\n",
        "    boxes = np.reshape(calc_rpns.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0)) \n",
        "    probs = np.array(x_class).transpose((0, 3, 1, 2)).reshape((-1)) \n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "    \n",
        "    \"\"\" delete boxes that are not existend (where x_max/y_max is lower than x_min/y_min) \"\"\"\n",
        "    to_del = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n",
        "    boxes = np.delete(boxes, to_del, 0)\n",
        "    probs = np.delete(probs, to_del, 0)\n",
        "\n",
        "    \"\"\" \n",
        "    apply non_max_suppression in order to create a choice for boxes based on \n",
        "    the threshold for IoU (threshold for max overlap of the bounding boxes) and\n",
        "    the score which imply the respective objectness scores\n",
        "    \"\"\"\n",
        "    indices = tf.image.non_max_suppression(boxes, probs, max_boxes, iou_threshold=0.5, score_threshold=0.4)\n",
        "    regions = [boxes[i] for i in indices]\n",
        "    \n",
        "    \n",
        "    return np.array(regions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c51bbfdb",
      "metadata": {
        "id": "c51bbfdb"
      },
      "outputs": [],
      "source": [
        "def train_step(model, inp, target, bbox, loss_function_class, loss_function_regr, optimizer):\n",
        "    \"\"\"\n",
        "    method to be executed each train step, for each image / bbox combination\n",
        "\n",
        "    Parameters:\n",
        "      model: model to be trained \n",
        "      inp: input image \n",
        "      target: label for the image with respective bounding box\n",
        "      bbox: bounding box coordinates for that specific image\n",
        "      loss_function_class: loss function for the classification task\n",
        "      loss_function_class: loss function for the regression task\n",
        "      optimizer: optimizer to be used for training\n",
        "\n",
        "    Returns:\n",
        "      loss: respective calculated loss\n",
        "      pred_class: prediction for the class of the image + bounding box\n",
        "      pred_regr: prediction for the bounding box itself\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        pred_class, pred_regr = model(inp)\n",
        "\n",
        "        loss_class = loss_function_class(target, pred_class)\n",
        "        loss_regr = loss_function_regr(bbox, pred_regr)\n",
        "        loss = loss_class  + loss_regr * 0.1\n",
        "        \n",
        "        gradients = tape.gradient(loss, model.trainable_variables, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
        "        \n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    return loss, pred_class, pred_regr\n",
        "  \n",
        "def test(model, test_data, loss_func_regr, loss_func_class):\n",
        "    \"\"\"\n",
        "    method to test the trained model on the never seen test dataset\n",
        "    Parameters:\n",
        "      model: trained model\n",
        "      test_data: testing dataset\n",
        "      loss_func_class: loss function used for training the classification\n",
        "      loss_func_regr : loss function used for training the regression\n",
        "    \n",
        "    Returns:\n",
        "      test_loss: identified loss for the test dataset\n",
        "    \"\"\"\n",
        "\n",
        "    test_loss_aggregator = []\n",
        "\n",
        "    for (input, target, box) in test_data:\n",
        "      pred_class, pred_regr = model(input)\n",
        "\n",
        "      test_loss_regr = loss_func_regr(box, pred_regr)\n",
        "      test_loss_class = loss_func_class(target, pred_class)\n",
        "      sample_test_loss = test_loss_regr * 0.1 + test_loss_class\n",
        "\n",
        "      test_loss_aggregator.append(sample_test_loss.numpy())\n",
        "\n",
        "    test_loss = tf.reduce_mean(test_loss_aggregator)\n",
        "\n",
        "    return test_loss\n",
        "\n",
        "def plot_bbox(img, box, label):\n",
        "    \"\"\"\n",
        "    method to plot a specific bounding box on the respective image \n",
        "    \n",
        "    Parameters:\n",
        "        img: original image \n",
        "        box: bounding box \n",
        "    \"\"\"\n",
        "    \n",
        "    x1, y1, x2, y2 = box\n",
        "    \n",
        "    if label == 0:\n",
        "        col= (0,0,255)\n",
        "    elif label == 1:\n",
        "        col= (0,255,0)\n",
        "    elif label == 2:\n",
        "        col= (255,0,0)\n",
        "    \n",
        "    cv2.rectangle(img, (int(x1), int(y1)), (int(x2), int(y2)), col, 1)\n",
        "\n",
        "    cv2.imshow(\"bbox_test\", img/255.0)\n",
        "    cv2.waitKey()  \n",
        "    cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a317685",
      "metadata": {
        "id": "9a317685"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "\"\"\" defining parameters for training \"\"\"\n",
        "\n",
        "num_epochs = 50\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_func_class = tf.keras.losses.CategoricalCrossentropy()\n",
        "loss_func_regr = tf.keras.losses.Huber()\n",
        "model = FasterRCNN()\n",
        "\n",
        "train_losses = []\n",
        "results = []\n",
        "\n",
        "for epoch in range(num_epochs):   \n",
        "    results.append([])\n",
        "    epoch_loss = []\n",
        "    i=0\n",
        "    for img_data in ds_train:\n",
        "        img, label, box = img_data \n",
        "        loss_curr, pred_class, pred_regr = train_step(model, img, label, box, loss_func_class, loss_func_regr, optimizer)\n",
        "        epoch_loss.append(loss_curr)\n",
        "        results[epoch].append((tf.squeeze(pred_regr), tf.squeeze(pred_class)))\n",
        "\n",
        "    train_losses.append(tf.reduce_mean(epoch_loss))\n",
        "\n",
        "    print(f'train_loss epoch {epoch}: {tf.reduce_mean(epoch_loss)}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = test(model, ds_test, loss_func_regr, loss_func_class)\n",
        "\n",
        "print(f'test_loss: {test_loss}')"
      ],
      "metadata": {
        "id": "-XIXlRGh7oOD"
      },
      "id": "-XIXlRGh7oOD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" plotting the before saved training loss \"\"\"\n",
        "\n",
        "DATADIR_RESULTS = os.path.join(DATADIR, \"results\")\n",
        "train_losses_bbox = np.load(f'/content/drive/MyDrive/Project_Face_Mask/results/train_losses_classif.npy', allow_pickle=True)\n",
        "\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"combined loss\")\n",
        "plt.plot(train_losses_bbox)"
      ],
      "metadata": {
        "id": "3lMyi8Oq7oSD"
      },
      "id": "3lMyi8Oq7oSD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "Plotting the bounding boxes\n",
        "At first the predicted and after the original\n",
        "\n",
        "doesn't work in colab, had to execute on local machine in jupyter\n",
        "\"\"\"\n",
        "epoch = 1\n",
        "\n",
        "for i, img_data in enumerate(ds_train.take(10)):\n",
        "    img, label, box = img_data\n",
        "    plot_bbox(img.numpy(), results[epoch][i][0].numpy(), np.argmax(results[epoch][i][1].numpy(), axis=0))\n",
        "    plot_bbox(img.numpy(), box.numpy(), label.numpy())"
      ],
      "metadata": {
        "id": "iEsCiJ-N5pFc"
      },
      "id": "iEsCiJ-N5pFc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "face_mask_detection_fasterrcnn_FINAL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}